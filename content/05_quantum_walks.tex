\chapter{Quantum walks}

Current technology is yet to produce a significant number of qubits (quantum bits) in a quantum processor, but many believe the amount will increase over the years. The first practical quantum algorithms to be run on these processors are likely to be the ones that use qubits sparingly. Quantum walking, the generalized version of classical random walking, is exactly this kind of algorithm. The number of qubits required to run a quantum walk on a graph is logarithmic in the number of vertices, making it a promising technique for the near future. Furthermore, by extending the quantum walk algorithm with a Grover search, it can be used to perform searching, which could be used for solving many of the computationally hard bioinformatical problems.

\section{Classical random walks}

Before introducing quantum computation and specifically quantum walks, I first overview classical random walks, based on the book Probability and Computing, written by Michael Mitzenmacher and Eli Upfal~\cite{MitzenmacherProbability}.

A \textit{random walk} is a stochastic process modeled by a particular type of Markov chain. While a variety of Markov chains exist, in this work, I use the following definition exclusively.

\begin{definition}[Markov chain]

A discrete time stochastic process $X_0, X_1, X_2, \dots$ on a finite state space $A$ is a Markov chain if it has the Markov property:

\begin{align*}
P(X_k = a_k \mid X_{k-1} = a_{k-1}, \dots, X_0 = a_0) = P(X_k = a_k \mid X_{k-1} = a_{k-1}) \addspace \forall a_0,\dots,a_k\in A.
\end{align*}

\end{definition}

Without loss of generality, we can assume, that $A = \{0,1,\dots,n\}$.

If the Markov chain is homogenous (time-invariant), the probability of moving from state $i \in A$ to state $j \in A$ is independent of $k$, and thus can be shortened the following way:

\begin{align*}
P(X_k = j \mid X_{k-1} = i) = p_{j\leftarrow{}i} = p_{j,i} \addspace \forall k \in \mathds{Z}^+.
\end{align*}

Where $p_{j,i}$ is called the \textit{transition probability} between states $i$ and $j$.

The \textit{transition matrix} $\mathbf{P}$ is formed by the transition probabilities.

\begin{align*}
    \mathbf{P}[j,i] = p_{j,i} 
\end{align*}

It follows, that for each column in $\mathbf{P}$, the sum is $1$.

\begin{align*}
    \sum\limits_{j=0}^{n}\mathbf{P}[j,i] = 1 \addspace \forall i \in \{0,\dots,n\}
\end{align*}

Let the \textit{probability distribution} of the process in the $t$-th step be $\pi_t$. Then, $\pi_t$ can be computed from the starting distribution $\pi_0$ using $\mathbf{P}$.

\begin{align*}
\pi_{t} = \mathbf{P}^t\pi_{0}
\end{align*}

The \textit{stationary distribution} ($\pi$) of the Markov chain is a distribution that does not change with a transition, i.e. $\pi = \mathbf{P}\pi$.

Markov chains can be represented using graphs. A directed, weighted graph $G(V,E)$ with weight function $w:E\rightarrow{}[0,1]$ represents a Markov chain, if $V=A$ and $w(i,j) = \mathbf{P}[j,i]$. If $\mathbf{P}[j,i] = 0$, then $\{i,j\} \not\in E$.

A random walk on graph $G$ starts from $X_0 = a_0$ and visits the vertices of the graph according to the states of the Markov-chain: $X_1 = a_1, X_2 = a_2, \dots$ .

Frequently studied characteristics of random walks are \textit{hitting time}~\cite{XiaReview} and \textit{mixing time}~\cite{MitzenmacherProbability}. Informally, hitting time describes how quickly can a vertex be reached from another vertex in the graph, while mixing time expresses how fast the walk reaches the stationary distribution, where the starting vertex can no longer be identified.

\begin{definition}[Hitting time] Let $h_{j,i}$ be the expected number of steps before node $j$ is visited in a random walk starting from node $i$.
Then, $h_{j,i}$ is given by the following recursive formula:
\begin{align*}
    h_{j,i} = \left\{
        \begin{array}{lr}
            1 + \sum\limits_{k\in{}A}p_{j,k}h_{k,i} & \text{if } i\neq{}j\\
            0 & \text{if } i=j
        \end{array}
    \right.
\end{align*}
\end{definition}

\begin{definition}[Mixing time] The smallest time index of the Markov chain, where the total variational distance between the current and the stationary distribution is not greater than a given $\varepsilon$. This measure still depends on the starting distrubiton $\pi_0$, so we take the maximum over all of the possible $\pi_0$ distributions.

\begin{align*}
m(\varepsilon) = \max\limits_{\pi_0}\{\min\{t : \sum\limits_{j=0}^{n}|\pi_t[j] - \pi[j]| \leq \varepsilon\}\}
\end{align*}

\end{definition}

\section{From classical to quantum}

In classical random walks, the walker moves from the current vertex via one of its outgoing edges, chosen randomly, weighted by the edge weights. This random choice can be interpreted as a (generalized) coin toss.

To formulate a quantum version of graph walking, we define the quantum coin, which will replace the classical concept of randomness with quantum superposition.

\section{Formulating the Quantum coin}

I used Renato Portugal's Quantum Walks and Search Algorithms~\cite{Portugal} book as a reference for the different types of coins presented in this section.

A \textit{quantum coin} is a quantum system, which behaves according to the postulates of quantum mechanics. It has a current state, represented by a state vector in a Hilbert space and a unitary time evolution operator, describing a coin toss.

After tossing the coin, the resulting coin state chooses the next step of the quantum walker. If there are $d$ outgoing edges to choose from, then the coin's state space must have $d$ orthonormal basis states, each corresponding to one of the possible edges. If the current state is one of the basis states, then the walker moves in that direction. However, in the quantum world, the coin can also be in a superposition, consisting of multiple basis states. This means that the walker will simultaneously move in all corresponding directions and occupy more than one vertex at the same time, resulting in the walker spreading over the graph in a superposition.

For the $d$ dimensional coin state, the corresponding coin flip operator is a $(d\times{}d)$ dimensional unitary matrix. Based on what the transition operator is, several types of coins can be defined. The following ones are typically used in quantum walks.

\subsection{Hadamard coin}

The Hadamard coin is the most commonly used quantum coin. It is defined by the Hadamard-matrix as a transition operator:

\begin{align*}
  \mathbf{H} = \frac{1}{\sqrt{2}}\begin{pmatrix}
      1 & 1  \\
      1 & -1
    \end{pmatrix}.
\end{align*}

If the starting coin state is $\ket{0}$, then flipping the coin once results in the following state:

\begin{align*}
 \mathbf{H}\ket{0} = \frac{1}{\sqrt{2}}\begin{pmatrix}
      1 & 1  \\
      1 & -1
    \end{pmatrix} \begin{pmatrix} 1 \\ 0 \end{pmatrix}
    = \frac{1}{\sqrt{2}} \begin{pmatrix} 1 \\ 1 \end{pmatrix} = \frac{1}{\sqrt{2}} \ket{0} + \frac{1}{\sqrt{2}} \ket{1}.
\end{align*}

If we measured the above coin, the probability of measuring $0$ is

\begin{align*}
P(0 \mid \frac{1}{\sqrt{2}} (\ket{0} + \ket{1})) =
\left\lvert\frac{1}{\sqrt{2}}\right\rvert^2 =
\frac{1}{2}.
\end{align*}

Similarly, if the starting coin state is $\ket{1}$, then flipping the coin once results in the following state:

\begin{align*}
   \mathbf{H}\ket{1} = \frac{1}{\sqrt{2}}\begin{pmatrix}
      1 & 1  \\
      1 & -1
    \end{pmatrix} \begin{pmatrix} 0 \\ 1 \end{pmatrix}
    = \frac{1}{\sqrt{2}} \begin{pmatrix} 1 \\ -1 \end{pmatrix} = \frac{1}{\sqrt{2}} \ket{0} - \frac{1}{\sqrt{2}} \ket{1}.
\end{align*}

The probability of measuring $1$ here is similarly

\begin{align*}
P(1 \mid \frac{1}{\sqrt{2}} (\ket{0} - \ket{1})) =
\left\lvert-\frac{1}{\sqrt{2}}\right\rvert^2 =
\frac{1}{2}.
\end{align*}

An unexpected feature of this coin comes from the fact, that the Hadamard-matrix is Hermitian (self-adjoint), i.e. $\mathbf{H}^{\dagger} = \mathbf{H}$, while also unitary, i.e. $\mathbf{H}^{\dagger} = \mathbf{H}^{-1}$, which results in $\mathbf{H}^{-1} = \mathbf{H}$, thus $\mathbf{H}\mathbf{H} = \mathbf{I}$. This means, that after flipping the coin twice without measuring it, it will return the coin state to its origin. For example, starting from $\ket{0}$:

\begin{align*}
 \mathbf{H}^2 \ket{0} = \mathbf{H}\frac{1}{\sqrt{2}}(\ket{0} + \ket{1}) = \frac{1}{2}(\ket{0} + \ket{1} + \ket{0} - \ket{1}) = \ket{0}.
\end{align*}

After the second flip, the probability of measuring $\ket{0}$ is $1$, due to the destructive interference between the two $\ket{1}$ probability amplitudes, demonstrating a significant contrast between classical and quantum walks.

\begin{definition}[$2^n$ dimensional Hadamard-coin]

A $2^n$ dimensional Hadamard-coin operator can be created by taking the tensor product of the $2$ dimensional Hadamard-coin $n$ times: $\mathbf{H}^{\otimes{}n}$.

\end{definition}

\subsection{Grover coin}

The Grover coin originates from Grover's search algorithm, where it is applied as the diffusion operator.

Let $\ket{D}$ be the following state:

\begin{align*}
\ket{D} = \mathbf{H}^{\otimes{}n}\ket{0} =
\frac{1}{\sqrt{2^n}} \sum\limits_{i=0}^{2^n-1} \ket{i}.
\end{align*}

Using $\ket{D}$, the Grover coin is the following unitary matrix:

\begin{align*}
    \mathbf{G} = 2\ket{D}\bra{D} - \mathbf{I}.
\end{align*}

If $N = 2^n$, then $\mathbf{G}$ unrolls to the following representation:

\begin{align*}
  \mathbf{G} = \begin{pmatrix}
      \frac{2}{N} - 1 & \frac{2}{N} & \dots  & \frac{2}{N} \\
      \frac{2}{N} & \frac{2}{N} - 1 & \dots  & \frac{2}{N} \\
      \vdots & \vdots & \ddots & \vdots \\
      \frac{2}{N} & \frac{2}{N} & \ddots & \frac{2}{N} - 1
    \end{pmatrix}.
\end{align*}

\subsection{Fourier coin}

In contrast to the Hadamard and Grover coins, the Fourier coin can be of any size, not just a power of 2. A size $N$ Fourier-coin, $F_N$ is defined by the matrix of the Quantum Fourier Transform:

\begin{align*}
\mathbf{F}[k,l] = \frac{1}{\sqrt{N}} \omega^{kl}
\end{align*}

where $\omega$ is the $N$-th root of unity,

\begin{align*}
\omega = e^{\frac{2\pi{}i}{N}}.
\end{align*}

$\mathbf{F}$ unrolls to the following representation:

\begin{align*}
\mathbf{F} = \frac{1}{\sqrt{N}}
\begin{pmatrix}
  1 & 1 & 1 & \dots & 1 \\
  1 & \omega & \omega^2 & \dots & \omega^{N-1} \\
  1 & \omega^2 & \omega^4 & \dots & \omega^{2(N-1)} \\
  \vdots & \vdots & \vdots & \ddots & \vdots \\
  1 & \omega^{N-1} & \omega^{2(N-1)} & \dots & \omega^{(N-1)(N-1)}
\end{pmatrix}
\end{align*}

\section{Quantum walks on the line}

Kempe introduces quantum walks from a physicist's perspective in~\cite{KempeIntroduction} using a particle characterised by its position on the line $\ket{x}$ and its spin state $\ket{s}$.

\subsection{State space}

\lineparagraph{Spin state}

The spin state is in $H_2$ with the basis states spin up and down:

\begin{align*}
\ket{\uparrow} = \ket{0}, \\
\ket{\downarrow} = \ket{1}.
\end{align*}

The spin state vector is then given by:

\begin{align*}
\ket{s} = s_0 \ket{\uparrow} + s_1 \ket{\downarrow}.
\end{align*}

\lineparagraph{Position state}

At the start of the walk the particle is at the origin $\ket{0}$ and the walking lasts for $N$ steps. The position state is in $H_{(2N+1)}$ with the following basis vectors corresponding to the possible positions on the line.

\begin{align*}
\{
\ket{-N},\ket{-(N-1)},\dots,
\ket{-1},\ket{0},\ket{1},\dots,
\ket{N-1},\ket{N}
\}
\end{align*}

I index the basis states using negative numbers to match the labels on the axis.

The position state vector is then given by:

\begin{align*}
    \ket{x} = \sum\limits_{i=-N}^{N}x_i\ket{i}.
\end{align*}

\lineparagraph{Composite state}

The composite state of the system, according to \hyperref[PostulateIV]{[PostulateIV]} is then

\begin{align*}
    \ket{x}\otimes{}\ket{s}.
\end{align*}

\subsection{Evolution}

The particle travels on the line based on its current spin state:
\begin{itemize}
\item If the current spin state is $\ket{0}$, the particle moves to the left, i.e. from position $\ket{i}$ the particle travels to position $\ket{i-1}$.
\item If the current spin state is $\ket{1}$, the particle moves to the right, i.e. from position $\ket{i}$ the particle travels to position $\ket{i+1}$.
\end{itemize}

This step is realised with the unitary matrix $\mathbf{S}$ which operates on the complete state of the system, $\ket{x}\otimes{}\ket{s}$ and is assembled from a left and a right shift operator acting on $\ket{x}$ and another operator for checking $\ket{s}$ compiled using tensor product.

\begin{definition}[Left shift operator]

To move from position $\ket{i}$ to the left ($\ket{i-1}$) the position vector is multiplied with the following $\mathbf{L}$ matrix, containing $1$'s above the diagonal. To keep $\mathbf{S}$ unitary, an unused transition must be added in the lower left corner (see \hyperref[PermutationMatricesTheorem]{Theorem \ref{PermutationMatricesTheorem})}.

\begin{align}
\label{LeftShift}
\mathbf{L} = \ket{N}\bra{-N} + \sum\limits_{i=-(N-1)}^{N} \ket{i-1}\bra{i} =
\left(
    \begin{array}{ccccc}
        0      & 1      & 0      & \cdots & 0      \\
               & \ddots & \ddots & \ddots & \vdots \\
        \vdots &        & \ddots & \ddots & 0      \\
        0      &        &        & \ddots & 1      \\
        1      & 0      & \cdots &        & 0
      \end{array}
\right)
\end{align}

For a given basis vector $\ket{j}$ only one of the summands in $\mathbf{L}$ is non-zero, where $i=j$, resulting in the required shift being performed.

\begin{align*}
\mathbf{L}\ket{j} = \ket{j-1}\bra{j}\ket{j} = \ket{j-1}
\end{align*}

\end{definition}

\begin{definition}[Right shift operator]

To move from position $\ket{i}$ to the right ($\ket{i+1}$) the position vector is multiplied with the following $\mathbf{R}$ matrix, containing $1$'s under the diagonal. To keep $\mathbf{S}$ unitary, an unused transition must be added in the top right corner (see \hyperref[PermutationMatricesTheorem]{Theorem \ref{PermutationMatricesTheorem})}.

\begin{align}
\label{RightShift}
\mathbf{R} = \ket{-N}\bra{N} + \sum\limits_{i=-N}^{N-1} \ket{i+1}\bra{i} =
\left(
    \begin{array}{ccccc}
        0      &        & \cdots & 0      & 1      \\
        1      & \ddots &        &        & 0      \\
        0      & \ddots & \ddots &        & \vdots \\
        \vdots & \ddots & \ddots & \ddots &        \\
        0      & \cdots & 0      & 1      & 0
      \end{array}
\right)
\end{align}

For a given basis vector $\ket{j}$ only one of the summands in $\mathbf{R}$ is non-zero, where $i=j$, resulting in the required shift being performed.

\begin{align*}
\mathbf{R}\ket{j} = \ket{j+1}\bra{j}\ket{j} = \ket{j+1}
\end{align*}

\end{definition}

\lineparagraph{Shift operator}

Using matrixes $L$ and $R$ operating on the position register $\ket{x}$ only, we construct a unitary operator $S$, which operates on the composite state of the system, $\ket{x} \otimes \ket{s}$, executing matrix $\mathbf{L}$ on $\ket{x}$ only when $\ket{s} = \ket{0}$ and matrix $\mathbf{R}$ only when $\ket{s}=\ket{1}$.

\begin{align}
\label{ShiftOperator}
  \mathbf{S} = \mathbf{L}\otimes\ket{0}\bra{0} + \mathbf{R}\otimes\ket{1}\bra{1}
\end{align}

The execution logic is as follows:

\begin{gather*}
    \mathbf{S}(\ket{x}\otimes{}\ket{s}) = \\
    (\mathbf{L}\otimes\ket{0}\bra{0} + \mathbf{R}\otimes\ket{1}\bra{1})(\ket{x}\otimes{}\ket{s}) = \\ (\mathbf{L}\otimes\ket{0}\bra{0})(\ket{x}\otimes{}\ket{s}) + (\mathbf{R}\otimes\ket{1}\bra{1})(\ket{x}\otimes{}\ket{s}) = \dots
\end{gather*}

using \hyperref[TensorMixedProduct]{[TensorMixedProduct]}:

\begin{gather*}
    \dots = \mathbf{L}\ket{x}\otimes(\ket{0}\bra{0}\ket{s}) + \mathbf{R}\ket{x}\otimes(\ket{1}\bra{1}\ket{s}) = \\
    \ket{x-1}\otimes{}s_0\ket{0} + \ket{x+1}\otimes{}s_1\ket{1} = \\
    s_0\ket{x-1, 0} + s_1\ket{x+1,1}.
\end{gather*}

\begin{itemize}
\item If the spin state was $\ket{s} = \ket{0}$ at the beginning, then $s_0=1$ and $s_1=0$, which means that the resulting system state is $\ket{x-1,0}$, which means that the particle shifted to the left, as designed.
\item If the spin state was $\ket{s} = \ket{1}$ at the beginning, then $s_0=0$ and $s_1=1$, which means that the resulting system state is $\ket{x+1,1}$, which means that the particle shifted to the right, also as intended.
\end{itemize}

Furthermore, the spin state can be any mixed state $s_0\ket{0} + s_1\ket{1}$ as well. In this case the particle will shift \textit{both} to the left and to the right, at the same time. When measured, the particle can be found in position $\ket{x-1}$ with probability $|s_0|^2$ and in position $\ket{x+1}$ with probability $|s_1|^2$.

In quantum graph walks, the walker can simultaneously explore multiple parallel paths in the graph, at the same time. With good design, this behaviour can be used to search the graph faster than in classical random graph walks.

\lineparagraph{Coin operator}

To inject the quantum superposition into the walk, the particle's spin state is transformed using any $2$ dimensional unitary matrix between shift operations. The Hadamard, Grover and Fourier coins mentioned earlier are commonly used as coin operators.

For any $C$ operator on the coin register, the unitary transform for the composite system is defined as follows:

\begin{align*}
    \mathbf{\hat{C}} = \mathbf{I} \otimes \mathbf{C}
\end{align*}

since the coin operator does not modify the position register.

\lineparagraph{Evolution operator}

Combining the shift operator and the coin operator together, we obtain the following evolution operator, defining one step of the quantum walk on the line. The step consists of flipping the coin once, then applying the shifting the walker's position accordingly, as follows:

\begin{align*}
    \mathbf{U} = \mathbf{S}\mathbf{\hat{C}} = \mathbf{S}(\mathbf{I}\otimes{}\mathbf{C})
\end{align*}

\subsection{Measurement}

To measure the probability of the particle being at position $\ket{i}$, the projective measurement operator acting on $\ket{x}$ is defined as $\mathbf{P_i} = \ket{i}\bra{i}$, in accordance with \hyperref[PostulateIIIProjective]{[PostulateIIIProjective]}.

Since the coin register need not be measured, we apply the identity operator on it, using $\mathbf{P_i} \otimes \mathbf{I}$ on the complete system to measure the particle's current position.

The probability of finding the particle in position $i$ is:

\begin{gather*}
    P(i | \ket{x}) = \bra{x,s}\mathbf{P_i}\otimes{}\mathbf{I}\ket{x,s} = \dots
\end{gather*}

using \hyperref[TensorMixedProduct]{[TensorMixedProduct]}:

\begin{gather*}
    \dots = \bra{x}\mathbf{P_i}\ket{x}\bra{s}\mathbf{I}\ket{s} =
    \bra{x}\mathbf{P_i}\ket{x}1 =
    \bra{x}\mathbf{P_i}\ket{x} =
    \bra{x}\ket{i}\bra{i}\ket{x} =
    |\bra{i}\ket{x}|^2 =
    |x_i|^2
\end{gather*}

\section{Generalization of Quantum Walks}

After presenting quantum walking on the line, I review and extend two approaches to generalize it in this chapter.

\begin{enumerate}
    \item \textbf{Section \ref{quantum-walk-multiple-2D-coins}: Use multiple two-dimensional coins}: In~\cite{Portugal}, Renato Portugal shows the generalization of quantum walks on a line to a two-dimensional grid, using a method with 2 two-dimensional coins. In this work, I prove how his method reduces to effectively two synchronous independent walks on the $x$ and $y$ axes. Then I improve his technique by generalizing to arbitrarily large dimensional grids in a more memory-efficient way than what would naturally follow from his description.
    \item \textbf{Section \ref{quantum-walk-one-highD-coin}: Use a single higher dimensional coin}: In~\cite{Portugal}, Renato Portugal describes the generalization of a quantum walk on a line to an arbitrary undirected graph and gives the necessary condition for creating the unitary transition matrix without proof. In this work, I generalize to directed graphs and give proof of the generalization of the condition using directed graphs.
\end{enumerate}

\subsection{Generalization using multiple independent 2 dimensional coins}
\label{quantum-walk-multiple-2D-coins}

In \cite{Portugal}, Renato Portugal defines the following method for Quantum Walking on a 2D grid:

Let the position state of the walker be $\ket{x,y}$ and the two coins $\ket{c_x}$, acting on the $x$ coordinate and $\ket{c_y}$, acting on the $y$ coordinate of the walker.

The shift operator moves the walker on the grid diagonally, according to the current state of the two coins, described by

\begin{align}
\label{Portugal2DShift}
    \mathbf{S}\ket{x,y}\ket{c_x}\ket{c_y} = \ket{x + (-1)^{c_x}, y + (-1)^{c_y}}\ket{c_x}\ket{c_y},
\end{align}

and the coin operator leaves the position state in place while flipping both coins at the same time, described by

\begin{align}
\mathbf{\hat{C}} = \mathbf{I} \otimes \mathbf{C_4} = \mathbf{I} \otimes (\mathbf{C_2} \otimes \mathbf{C_2}).
\end{align}

\lineparagraph{Issues with this method}

In \ref{Portugal2DShift}, we can see how the matrix $\mathbf{S}$ will quickly increase in size, as further dimensions are
added to the equation. In 2D, if the walker takes $N$ steps, the size of $\mathbf{S}$ is

\begin{align*}
(2N+1)^2(2N+1)^22^22^2 = 16(2N+1)^4 = O(N^4).
\end{align*}

To increase the dimension count, one would naturally append more coordinates to the composite position state and add further coins, for example in 3D $\mathbf{S}$ would become

\begin{align*}
    \mathbf{S}\ket{x,y,z}\ket{c_x}\ket{c_y}\ket{c_z} =
    \ket{x + (-1)^{c_x}, y + (-1)^{c_y}, z + (-1)^{c_z}}\ket{c_x}\ket{c_y}\ket{c_z},
\end{align*}

For a dimension count $d$, the size of $\mathbf{S}$ is exponential in $d$.

\begin{align*}
((2N+1)^2)^d(2^2)^d = (4(2N+1)^2)^d = O(N^{2d}).
\end{align*}

\lineparagraph{My improvements}

Since in \ref{Portugal2DShift} the coordinates of the walker are updated independently by the separate coins, I was able to disassemble $\mathbf{S}$ into smaller matrices, using the properties of the tensor product.

To do this, in what follows, I first define $\mathbf{S}$ in 2D explicitly (as opposed to the implicit definition in \ref{Portugal2DShift}, stating only how $\mathbf{S}$ updates the state of the system). I will be using the matrices $\mathbf{L}$ defined by Equation (\ref{LeftShift}) and $\mathbf{R}$ defined by Equation (\ref{RightShift}).

Notice that $\mathbf{R}$ increases the walker's coordinates on the line, while $\mathbf{L}$ decreases them. This means, that on the $y$ axis $\mathbf{R}$ acts by moving the walker up, and $\mathbf{L}$ acts by moving the walker down.

When the coins are in the state $\ket{c_x,c_y} = \ket{0,0}$, the walker moves up and to the right. This movement is captured by $\mathbf{S}_{0,0}$, acting on the position state:

\begin{align*}
    \mathbf{S}_{0,0} =  \mathbf{R} \otimes \mathbf{R}
\end{align*}

The other three $\mathbf{S}_{c_x,c_y}$ matrices are defined similarly: 

\begin{align*}
    \mathbf{S}_{0,1} =  \mathbf{R} \otimes \mathbf{L}\\
    \mathbf{S}_{1,0} =  \mathbf{L} \otimes \mathbf{R}\\
    \mathbf{S}_{1,1} =  \mathbf{L} \otimes \mathbf{L}
\end{align*}

Then, I assemble $\mathbf{S}$ using $\mathbf{S}_{0,0}$, $\mathbf{S}_{0,1}$, $\mathbf{S}_{1,0}$ and $\mathbf{S}_{1,1}$ the following way:
\begin{itemize}
\item $\mathbf{S}_{0,0}$ acts only when $\ket{c_x,c_y} = \ket{0,0}$,
\item $\mathbf{S}_{0,1}$ acts only when $\ket{c_x,c_y} = \ket{0,1}$,
\item $\mathbf{S}_{1,0}$ acts only when $\ket{c_x,c_y} = \ket{1,0}$, and finally
\item $\mathbf{S}_{1,1}$ acts only when $\ket{c_x,c_y} = \ket{1,1}$.
\end{itemize}

Using the same method as in Equation (\ref{ShiftOperator}) I arrive at:

\begin{align*}
    \mathbf{S} =
    \mathbf{S}_{0,0} \otimes \ket{0,0}\bra{0,0} + \\
    \mathbf{S}_{0,1} \otimes \ket{0,1}\bra{0,1} + \\
    \mathbf{S}_{1,0} \otimes \ket{1,0}\bra{1,0} + \\
    \mathbf{S}_{1,1} \otimes \ket{1,1}\bra{1,1}\phantom{+}
\end{align*}

After substituting the $\mathbf{S}_{c_x,c_y}$ matrices in:

\begin{align*} 
    \mathbf{S} =   
    (\mathbf{R} \otimes \mathbf{R}) \otimes \ket{0,0}\bra{0,0} + \\
    (\mathbf{R} \otimes \mathbf{L}) \otimes \ket{0,1}\bra{0,1} + \\
    (\mathbf{L} \otimes \mathbf{R}) \otimes \ket{1,0}\bra{1,0} + \\
    (\mathbf{L} \otimes \mathbf{L}) \otimes \ket{1,1}\bra{1,1}\phantom{+}
\end{align*}

Let us name the coin state $\ket{0}$ heads and the coin state $\ket{1}$ tails. Then, using the following equalities and introducing matrices $\mathbf{H}$ and $\mathbf{T}$, as shorthands:

\begin{align*} 
    \ket{0,0}\bra{0,0} = (\ket{0}\bra{0}) \otimes (\ket{0}\bra{0}) = \mathbf{H} \otimes \mathbf{H} \\
    \ket{0,1}\bra{0,1} = (\ket{0}\bra{0}) \otimes (\ket{1}\bra{1}) = \mathbf{H} \otimes \mathbf{T} \\
    \ket{1,0}\bra{1,0} = (\ket{1}\bra{1}) \otimes (\ket{0}\bra{0}) = \mathbf{T} \otimes \mathbf{H} \\
    \ket{1,1}\bra{1,1} = (\ket{1}\bra{1}) \otimes (\ket{1}\bra{1}) = \mathbf{T} \otimes \mathbf{T}
\end{align*}

I arrive at:

\begin{align*} 
    \mathbf{S} =   
    (\mathbf{R} \otimes \mathbf{R}) \otimes (\mathbf{H} \otimes \mathbf{H}) + \\
    (\mathbf{R} \otimes \mathbf{L}) \otimes (\mathbf{H} \otimes \mathbf{T}) + \\
    (\mathbf{L} \otimes \mathbf{R}) \otimes (\mathbf{T} \otimes \mathbf{H}) + \\
    (\mathbf{L} \otimes \mathbf{L}) \otimes (\mathbf{T} \otimes \mathbf{T})\phantom{+}
\end{align*}

Then, using \hyperref[TensorMixedProduct]{[TensorMixedProduct]} I inflate the equation with (appropriately sized) $\mathbf{I}$ matrices:

\begin{align*} 
    \mathbf{S} =   
    ((\mathbf{R} \otimes \mathbf{I})(\mathbf{I} \otimes \mathbf{R})) \otimes ((\mathbf{H} \otimes \mathbf{I}) (\mathbf{I} \otimes \mathbf{H})) + \\
    ((\mathbf{R} \otimes \mathbf{I})(\mathbf{I} \otimes \mathbf{L})) \otimes ((\mathbf{H} \otimes \mathbf{I}) (\mathbf{I} \otimes \mathbf{T})) + \\
    ((\mathbf{L} \otimes \mathbf{I})(\mathbf{I} \otimes \mathbf{R})) \otimes ((\mathbf{T} \otimes \mathbf{I}) (\mathbf{I} \otimes \mathbf{H})) + \\
    ((\mathbf{L} \otimes \mathbf{I})(\mathbf{I} \otimes \mathbf{L})) \otimes ((\mathbf{T} \otimes \mathbf{I}) (\mathbf{I} \otimes \mathbf{T}))\phantom{+}
\end{align*}

At this point, I introduce a few aliases to make the equation more manageable. Notice, how for example $\mathbf{I} \otimes \mathbf{R}$ is acting on the 2D position state, but only updating the $y$ coordinate to move the walker upward. This
gives a way to naturally define $\mathbf{S}_{\text{up}} = \mathbf{I} \otimes \mathbf{R}$, and similarly:

\begin{align*} 
\mathbf{S}_{\text{right}} = \mathbf{R} \otimes \mathbf{I} \\
\mathbf{S}_{\text{left}} = \mathbf{L} \otimes \mathbf{I} \\
\mathbf{S}_{\text{up}} = \mathbf{I} \otimes \mathbf{R} \\
\mathbf{S}_{\text{down}} = \mathbf{I} \otimes \mathbf{L} \\
\end{align*}

At the same time, for example $\mathbf{I} \otimes \mathbf{H}$ is acting on the composite coin state, but only checking if the second coin's state is heads, which is the coin for the $y$ axis. This gives a way to naturally define $\mathbf{H}_y = \mathbf{I} \otimes \mathbf{H}$, and similarly:

\begin{align*} 
\mathbf{H}_x = \mathbf{H} \otimes \mathbf{I} \\
\mathbf{T}_x = \mathbf{T} \otimes \mathbf{I} \\
\mathbf{H}_y = \mathbf{I} \otimes \mathbf{H} \\
\mathbf{T}_y = \mathbf{I} \otimes \mathbf{T} \\
\end{align*}

Substituting all of the aliases:

\begin{align*} 
    \mathbf{S} =   
    (\mathbf{S}_{\text{right}}\mathbf{S}_{\text{up}}) \otimes (\mathbf{H}_x\mathbf{H}_y) + \\
    (\mathbf{S}_{\text{right}}\mathbf{S}_{\text{down}}) \otimes (\mathbf{H}_x\mathbf{T}_y) + \\
    (\mathbf{S}_{\text{left}}\mathbf{S}_{\text{up}}) \otimes (\mathbf{T}_x\mathbf{H}_y) + \\
    (\mathbf{S}_{\text{left}}\mathbf{S}_{\text{down}}) \otimes (\mathbf{T}_x\mathbf{T}_y)\phantom{+}
\end{align*}

Then, using \hyperref[TensorMixedProduct]{[TensorMixedProduct]} again, I arrive at:

\begin{align*} 
    \mathbf{S} =   
    (\mathbf{S}_{\text{right}} \otimes \mathbf{H}_x) (\mathbf{S}_{\text{up}} \otimes \mathbf{H}_y) + \\
    (\mathbf{S}_{\text{right}} \otimes \mathbf{H}_x) (\mathbf{S}_{\text{down}} \otimes \mathbf{T}_y) + \\
    (\mathbf{S}_{\text{left}} \otimes \mathbf{T}_x) (\mathbf{S}_{\text{up}} \otimes \mathbf{H}_y) + \\
    (\mathbf{S}_{\text{left}} \otimes \mathbf{T}_x) (\mathbf{S}_{\text{down}} \otimes \mathbf{T}_y)\phantom{+}
\end{align*}

Then using the distributive property of matrix multiplication with respect to matrix addition I finally arrive at:

\begin{align*} 
    \mathbf{S} =   
    ((\mathbf{S}_{\text{right}} \otimes \mathbf{H}_x) + (\mathbf{S}_{\text{left}} \otimes \mathbf{T}_x))
    ((\mathbf{S}_{\text{up}} \otimes \mathbf{H}_y) + (\mathbf{S}_{\text{down}} \otimes \mathbf{T}_y))
\end{align*}

We can see from the equation above, that $\mathbf{S}$ is actually the product of two shift operators, one only
acting on the $x$ coordinate using the first coin's state, the other acting only on the $y$ coordinate, according to the second
coin's state.

\begin{align*} 
    \mathbf{S}_x = (\mathbf{S}_{\text{right}} \otimes \mathbf{H}_x) + (\mathbf{S}_{\text{left}} \otimes \mathbf{T}_x) \\
    \mathbf{S}_y = (\mathbf{S}_{\text{up}} \otimes \mathbf{H}_y) + (\mathbf{S}_{\text{down}} \otimes \mathbf{T}_y) \\
\end{align*}    

\begin{align*}
    \mathbf{S} = \mathbf{S}_x\mathbf{S}_y
\end{align*}

This proves, that the walk on the 2D grid decomposes into two independent line walks on the axes, since $\mathbf{S}_x$ only touches the $x$ coordinate and the first coin, while $\mathbf{S}_y$ only touches the $y$ coordinate and the second coin and there is no entanglement between the registers of the $x$ and $y$ axes. Using this fact, we can simply simulate two independent quantum walks on the line in parallel, or sequentially, using the same registers, which wastly decreases the memory needs of the algorithm. Running in parallel, the memory needs is now $d(4(2N+1)^2) = O(dN^2)$, or running sequentially $(4(2N+1))^2 = O(N^2)$, however the latter uses $dN$ steps, instead of $N$, and $d$ measurements, instead of $1$.

\subsection{Generalization using a single higher dimensional coin}
\label{quantum-walk-one-highD-coin}

Using this method, we generalize quantum walking to arbitrary directed graphs. To do so, we first generalize to regular graphs, then discuss how non-regular graphs can be regularized.

In a $d$-regular graph, the vertices have $d$ neighbours, meaning the walker must choose from $d$ possible directions at every step. Thus the coin is $d$ dimensional. Using this idea as a starting point, we can reverse engineer the generalized walk from the walk on the line by starting from the evolution operator, which assumes nothing about the given graph or coin.

\begin{align*}
    \mathbf{U} = \mathbf{S} \mathbf{\hat{C}} = \mathbf{S} (\mathbf{I} \otimes \mathbf{C})
\end{align*}

In this equation, $\mathbf{C}$ can be any $d$-dimensional unitary matrix. The definition of $\mathbf{S}$ requires more thought, since $\mathbf{S}$ has to encode the graph's structure, while also satisfying \hyperref[PostulateII]{[PostulateII]}.

In one dimension, $\mathbf{S}$ was defined the following way:

\begin{align*}
\mathbf{S} = \mathbf{L}\otimes\ket{0}\bra{0} + \mathbf{R}\otimes\ket{1}\bra{1}.
\end{align*}

$\ket{0}\bra{0}$ and $\ket{1}\bra{1}$ were matrices that checked the current state of the coin and ''activated'' the transition $\mathbf{L}$ or $\mathbf{R}$ accordingly. In $d$ dimensions, the coin has $d$ possible states, i.e. ''sides''

\begin{align*}
    \{\ket{0}, \ket{1}, \dots, \ket{d-1}\},
\end{align*}

which means $\mathbf{S}$ will be constructed using $d$ transition matrices, describing the graph's structure

\begin{align*}
    \mathbf{S} = \mathbf{S}_0\ket{0}\bra{0} + \mathbf{S}_1\ket{1}\bra{1} + \dots + \mathbf{S}_{d-1}\ket{d-1}\bra{d-1}.
\end{align*}

In the 1 dimensional case $\mathbf{L}$ and $\mathbf{R}$ described stepping to the left and to the right, which are
the directed edges of the line graph and $\mathbf{L} + \mathbf{R}$ is the adjacency matrix of the line graph. To generalize this, $\mathbf{S}_0 + \mathbf{S}_1 + \dots + \mathbf{S}_{d-1}$ is going to be the adjacency matrix of the $d$-regular graph.

The question is how do we construct the matrices $\mathbf{S}_0, \mathbf{S}_1, \dots, \mathbf{S}_{d-1}$ from a given adjacency matrix of a $d$-regular graph? It turns out, that in order for $\mathbf{S}$ to satisfy \hyperref[PostulateII]{[PostulateII]}, there are strict rules on how these $\mathbf{S}_i$ matrices can be defined.

It seems to be a well-known fact in the literature, that for $d$-regular undirected graphs with a valid edge coloring using $d$ colors, a possible choice for the sides of the coin correspond to the colorsets of the edges. This means, that the $S_i$ adjacency matrix contains all of the edges that have the $i$th color assigned to them, in both directions (i.e. $S_i$ is symmetric). This is also mentioned in \cite{Portugal}, however no proof is given in this book or any other books and articles I have found during research.

In the following section, I present a more generalized theorem for directed $d$-regular graphs formulated and proven by me. Then I discuss how the special case of the theorem for undirected graphs gives the edge coloring as a result.

\begin{theorem}
\label{PermutationMatricesTheorem}
Given a coined quantum walk on a directed, $d$-regular graph $G$, in the shift operator of the walk: $\mathbf{S} = \sum\limits_{i=0}^{d-1} \mathbf{S}_i \otimes{} \ket{i}\bra{i}$, assuming the $\mathbf{S}_{i}$ are nonnegative, real matrices, it follows that they are \textbf{permutation} matrices.
\end{theorem}

\textit{Proof.}

According to \hyperref[PostulateII]{[PostulateII]}, $\mathbf{S}$ must be unitary.

According to \hyperref[Unitary]{[Unitary]} the columns of $\mathbf{S}$ form an orthonormal basis. This means, that the inner product of any two different columns is $0$.

Let $\mathbf{S}$ be a matrix of size $(N\times{}N)$. Then

\begin{align*}
    (\mathbf{S}\ket{k})^{\dagger}(\mathbf{S}\ket{j}) = 0 \addspace{} \forall{}j\neq{}k,\ 0\leq{}j,k\leq{}N.
\end{align*}

Since both the $\mathbf{S}_i$ matrices and the $\ket{i}\bra{i}$ matrices contain only non-complex, non-negative values, this means that $\mathbf{S}$ contains also only non-complex, non-negative values. Thus the only way the inner product of two different columns can be $0$ is if the columns don't contain non-zero values in the same row.

From this observation follows, that for each individual $S_i$ matrix, no two different columns can contain non-zero values in the same row. If there were two different columns in $S_i$, that contained non-zero values in the same row, then that would result in $\mathbf{S}_i \otimes \ket{i}\bra{i}$ containing two different columns containing non-zero values in the same row, which would result in $\mathbf{S}$ containing two different columns containing non-zero values in the same row (since no matrices contain negative or complex values), which is a contradiction.

Using \hyperref[Unitary]{[Unitary]}, the rows of $\mathbf{S}$ also form an orthonormal basis. With similar reasoning, it can be proven that for each individual $S_i$ matrix, no two different rows can contain non-zero values in the same column.

From these two observations follows, that each matrix $\mathbf{S}_i$ contains exactly one non-zero value in each row and also each column. Adding the fact, that the rows and columns of $\mathbf{S}$ are normalized, means that this non-zero value must always be a $1$, resulting in the $\mathbf{S}_i$ being \textbf{permutation} matrices.

\begin{flushright}
$\square{}.$
\end{flushright}

When this theorem is applied to undirected graphs, the adjacency matrices $\mathbf{S}_i$ can be contructed to be symmetric (since the graph's adjacency matrix is also symmetric) and in this case they correspond to a valid edge coloring using $d$ colors (since if $\mathbf{S}_i[j,k] = 1$, then also $\mathbf{S}_i[k,j] = 1$ due to symmetry, and the $\{k,j\}$ edge has the color $i$, while no other edges of vertex $j$ or $k$ have the $i$th color, since $\mathbf{S}_i$ is a permutation matrix).

Applying Vizing's theorem to $d$-regular graphs, the graphs can be categorized into two classes:
\begin{itemize}
\item \textbf{Class 1 $d$-regular graphs}: Their edge-chromatic number is $d$. The construction defined in this section works for these graphs.
\item \textbf{Class 2 $d$-regular graphs}: Their edge-chromatic number is $d+1$. \cite{Portugal} seems to state, that for these types of graphs, the position-coin notation does not give a way to construct a quantum walk on them (and the arc notation shall be used), however using Theorem (\ref{PermutationMatricesTheorem}) if we extend the method to directed graphs, it can be possible to do so. For example, the triangle is $2$-regular, but its edge-chromatic number is $3$. However, if we direct the edges both ways, we suddenly arrive at two cycles of length 3, for which the adjacancy matrices are permutation matrices, allowing for the construction of a unitary evolution operator.

\end{itemize}
